import argparse
import json
import logging

from pathlib import Path
from tqdm import tqdm

import comet
import h5py
import numpy as np
import torch

from efficient_reranking.lib import utils


class MissingArgumentError(ValueError):
    pass

def main(args):
    torch.manual_seed(args.seed)

    work_dir = Path(args.work_dir) / args.split
    work_dir.mkdir(parents=True, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if args.comet_repo:
        comet_base_name = args.comet_repo.split("/")[-1]
        model_path = comet.download_model(args.comet_repo)
        model = comet.load_from_checkpoint(model_path).eval()
    elif args.comet_path:
        comet_base_name = args.comet_path.split("/")[-3]
        model = comet.load_from_checkpoint(args.comet_path)
    else:
        raise MissingArgumentError("Must provide --comet_repo or --comet_path.")

    if args.mc_dropout:
        output_path_base = work_dir / (utils.COMET_SCORES_FILENAME_BASE + comet_base_name + f"_dropout_{args.seed}.h5")
    else:
        output_path_base = work_dir / (utils.COMET_SCORES_FILENAME_BASE + comet_base_name + ".h5")
    output_path = output_path_base.with_suffix(".h5")

    if output_path.exists():
        raise ValueError(f"Output file {output_path} already exists.")

    # TODO (julius): This logs to stdout twice for some reason
    utils.configure_logger("score_comet.py", output_path_base.with_suffix(".log"))

    logging.info(f"Evaluating candidates with COMET...")

    candidates_path = work_dir / (utils.CANDIDATES_FILENAME + ".h5")
    data_path = Path(args.data_dir) / "jsonl" / f"{args.split}.jsonl"
    data_lines = open(data_path).readlines()

    with (h5py.File(output_path, "a") as output_file,
          h5py.File(candidates_path) as candidates_file):
        candidates_text_h5ds = candidates_file[utils.CANDIDATES_TEXT_H5DS_NAME]
        if utils.COMET_SCORES_H5DS_NAME in output_file:
            scores_h5ds = output_file[utils.COMET_SCORES_H5DS_NAME]
        else:
            scores_h5ds = output_file.create_dataset(
                utils.COMET_SCORES_H5DS_NAME,
                candidates_text_h5ds.shape,
                float
            )

        data_idxs = []
        inputs = []
        logging.info("Preparing inputs...")
        for i, data_line in enumerate(tqdm(data_lines)):
            if scores_h5ds[i, 0] and not args.overwrite:
                continue
            data = json.loads(data_line)
            src = data["src"]
            tgts = [candidates_text_h5ds[i, j].decode() for j in range(candidates_text_h5ds.shape[1])]
            # Skip missing candidates
            if all(not tgt for tgt in tgts):
                continue
            data_idxs.append(i)
            for tgt in tgts:
                inputs.append({"src": src, "mt": tgt})

        if not inputs:
            return
        logging.info("Scoring...")
        with torch.no_grad():
            result = model.predict(samples=inputs, batch_size=args.comet_batch_size, mc_dropout=args.mc_dropout)
        logging.info("Writing results to file...")
        scores = np.matrix(result.scores).reshape(-1, scores_h5ds.shape[1])

        for result_idx, data_idx in enumerate(data_idxs):
            scores_h5ds[data_idx] = scores[result_idx]


    logging.info(f"Finished.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "data_dir", help="Data directory generated by the pipeline from vilem/scripts.")

    parser.add_argument(
        "split", type=str, help="Data split. Either 'dev' or 'test'.")

    parser.add_argument(
        "work_dir", help="Working directory for all steps. "
                         "Will be created if doesn't exist.")

    parser.add_argument(
        "--comet_repo", help="Huggingface COMET model name. Must pass --comet_repo or --comet_path")

    parser.add_argument(
        "--comet_path", help="COMET model directory. Must pass --comet_repo or --comet_path")

    parser.add_argument(
        "--seed", type=int, default=0, help="Random seed for PyTorch.")

    parser.add_argument(
        "--comet_batch_size", type=int, default=128, help="COMET batch size.")

    parser.add_argument(
        "--mc_dropout", action="store_true", help="Activate MC dropout.")


    args = parser.parse_args()
    main(args)