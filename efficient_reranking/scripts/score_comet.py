import argparse
import json
import logging
import os
import sys

import numpy as np

# Logging format borrowed from Fairseq.
logging.basicConfig(
    stream=sys.stdout,
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S",
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s")

from pathlib import Path

import comet
import h5py
import torch

from tqdm import tqdm
from transformers import GenerationConfig

from efficient_reranking.lib import datasets, generation, utils


class MissingArgumentError(ValueError):
    pass

def main(args):
    torch.manual_seed(0)
    work_dir = Path(args.work_dir) / args.split
    work_dir.mkdir(parents=True, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    logging.info(f"Evaluating candidates with COMET...")

    if args.comet_repo:
        comet_base_name = args.comet_repo.split("/")[-1]
        model_path = comet.download_model(args.comet_repo)
        model = comet.load_from_checkpoint(model_path).eval()
    elif args.comet_path:
        comet_base_name = os.path.splitext(args.comet_path.split("/")[-1])[0]
        model = comet.load_from_checkpoint(args.comet_path)
    else:
        raise MissingArgumentError("Must provide --comet_repo or --comet_path.")

    output_path = work_dir / (utils.COMET_SCORES_FILENAME_BASE + comet_base_name + ".h5")
    if output_path.exists():
        if args.overwrite:
            logging.info(f"Output file {output_path} exists but overwriting.")
        else:
            logging.info(f"Output file {output_path} exists, aborting. Use --overwrite to overwrite.")

    candidates_path = work_dir / (utils.CANDIDATES_FILENAME + ".h5")
    data_path = Path(args.data_dir) / "jsonl" / f"{args.split}.jsonl"
    data_lines = open(data_path).readlines()

    with (h5py.File(output_path, "a") as output_file,
          h5py.File(candidates_path) as candidates_file):
        candidates_text_h5ds = candidates_file[utils.CANDIDATES_TEXT_H5DS_NAME]
        if utils.COMET_SCORES_H5DS_NAME in output_file:
            scores_h5ds = output_file[utils.COMET_SCORES_H5DS_NAME]
        else:
            scores_h5ds = output_file.create_dataset(
                utils.COMET_SCORES_H5DS_NAME,
                candidates_text_h5ds.shape,
                float
            )

        data_idxs = []
        inputs = []
        logging.info("Preparing inputs...")
        for i, data_line in enumerate(tqdm(data_lines)):
            if scores_h5ds[i, 0] and not args.overwrite:
                continue
            data_idxs.append(i)
            data = json.loads(data_line)
            src = data["src"]
            tgts = [candidates_text_h5ds[i, j].decode() for j in range(candidates_text_h5ds.shape[1])]
            # Skip missing candidates
            if all(not tgt for tgt in tgts):
                continue
            for tgt in tgts:
                inputs.append({"src": src, "mt": tgt})

        logging.info("Scoring...")
        result = model.predict(samples=inputs, batch_size=args.comet_batch_size)
        scores = np.matrix(result.scores).reshape(-1, scores_h5ds.shape[1])

        for result_idx, data_idx in enumerate(data_idxs):
            scores_h5ds[data_idx] = scores[result_idx]

        # # Old code that didn't pass all inputs into COMET at once, which is
        # # much faster
        # for i, data_line in enumerate(tqdm(data_lines)):
        #     if scores_h5ds[i, 0] and not args.overwrite:
        #         continue
        #     data = json.loads(data_line)
        #     src = data["src"]
        #     tgts = [candidates_text_h5ds[i, j].decode() for j in range(candidates_text_h5ds.shape[1])]
        #     # Skip missing candidates
        #     if all(not tgt for tgt in tgts):
        #         continue
        #     # inputs = model.encoder.prepare_sample([src] + tgts).to(device)
        #     data = [
        #         {"src": src, "mt": tgt}
        #         for tgt in tgts
        #     ]
        #     result = model.predict(samples=data, batch_size=128)
        #     scores_h5ds[i] = result.scores

    logging.info(f"Finished.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "data_dir", help="Data directory generated by the pipeline from vilem/scripts.")

    parser.add_argument(
        "split", type=str, help="Data split. Either 'dev' or 'test'.")

    parser.add_argument(
        "work_dir", help="Working directory for all steps. "
                         "Will be created if doesn't exist.")

    parser.add_argument(
        "--comet_repo", help="Huggingface COMET model name. Must pass --comet_repo or --comet_path")

    parser.add_argument(
        "--comet_path", help="COMET model directory. Must pass --comet_repo or --comet_path")

    parser.add_argument(
        "--overwrite", action="store_true", help="Overwrite existing data.")

    parser.add_argument(
        "--seed", type=int, default=0, help="Random seed for PyTorch.")

    parser.add_argument(
        "--comet_batch_size", type=int, default=128, help="COMET batch size.")


    args = parser.parse_args()
    main(args)
