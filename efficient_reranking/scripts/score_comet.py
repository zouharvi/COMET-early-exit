import argparse
import json
import logging

from pathlib import Path
from tqdm import tqdm

import comet
import h5py
import numpy as np
import torch

from efficient_reranking.lib import utils


class MissingArgumentError(ValueError):
    pass

def main(args):
    torch.manual_seed(args.seed)

    work_dir = Path(args.work_dir) / args.split
    work_dir.mkdir(parents=True, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    if args.comet_repo:
        comet_base_name = args.comet_repo.split("/")[-1]
        model_path = comet.download_model(args.comet_repo)
        model = comet.load_from_checkpoint(model_path).eval()
    elif args.comet_path:
        comet_base_name = args.comet_path.split("/")[-3]
        model = comet.load_from_checkpoint(args.comet_path)
    else:
        raise MissingArgumentError("Must provide --comet_repo or --comet_path.")

    if args.mc_dropout:
        output_path_base = work_dir / (utils.COMET_SCORES_FILENAME_BASE + comet_base_name + f"_dropout_{args.seed}.h5")
    else:
        output_path_base = work_dir / (utils.COMET_SCORES_FILENAME_BASE + comet_base_name + ".h5")
    output_path = output_path_base.with_suffix(".h5")

    if output_path.exists() and not args.overwrite:
        raise ValueError(f"Output file {output_path} already exists.")

    # TODO (julius): This logs to stdout twice for some reason
    utils.configure_logger("score_comet.py", output_path_base.with_suffix(".log"))

    logging.info(f"Evaluating candidates with COMET...")

    candidates_path = work_dir / (utils.CANDIDATES_FILENAME + ".h5")
    data_path = Path(args.data_dir) / "jsonl" / f"{args.split}.jsonl"
    data_lines = open(data_path).readlines()

    with (h5py.File(output_path, "w") as output_file,
          h5py.File(candidates_path) as candidates_file):
        candidates_text_h5ds = candidates_file[utils.CANDIDATES_TEXT_H5DS_NAME]
        if utils.COMET_SCORES_H5DS_NAME in output_file:
            scores_h5ds = output_file[utils.COMET_SCORES_H5DS_NAME]
        else:
            scores_h5ds = output_file.create_dataset(
                utils.COMET_SCORES_H5DS_NAME,
                candidates_text_h5ds.shape,
                float
            )

        inputs = []
        all_num_cands = []
        logging.info("Preparing inputs...")
        for i, data_line in enumerate(tqdm(data_lines)):
            data = json.loads(data_line)
            src = data["src"]
            tgts = []
            for j in range(candidates_text_h5ds.shape[1]):
                if not candidates_text_h5ds[i, j]:
                    break
                tgts.append(candidates_text_h5ds[i, j].decode())
            all_num_cands.append(len(tgts))
            for tgt in tgts:
                inputs.append({"src": src, "mt": tgt})

        if not inputs:
            return
        logging.info("Scoring...")
        with torch.no_grad():
            result = model.predict(samples=inputs, batch_size=args.comet_batch_size, mc_dropout=args.mc_dropout)
        logging.info("Writing results to file...")

        seen = 0
        for i, num_cands in enumerate(all_num_cands):
            scores_row = np.zeros(candidates_text_h5ds.shape[1])
            scores_row[:num_cands] = result.scores[seen:seen+num_cands]
            scores_h5ds[i] = scores_row
            seen += num_cands

        # # Old code that didn't pass all inputs into COMET at once, which is
        # # much faster
        # for i, data_line in enumerate(tqdm(data_lines)):
        #     if scores_h5ds[i, 0] and not args.overwrite:
        #         continue
        #     data = json.loads(data_line)
        #     src = data["src"]
        #     tgts = [candidates_text_h5ds[i, j].decode() for j in range(candidates_text_h5ds.shape[1])]
        #     # Skip missing candidates
        #     if all(not tgt for tgt in tgts):
        #         continue
        #     # inputs = model.encoder.prepare_sample([src] + tgts).to(device)
        #     data = [
        #         {"src": src, "mt": tgt}
        #         for tgt in tgts
        #     ]
        #     result = model.predict(samples=data, batch_size=128)
        #     scores_h5ds[i] = result.scores

    logging.info(f"Finished.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "data_dir", help="Data directory generated by the pipeline from vilem/scripts.")

    parser.add_argument(
        "split", type=str, help="Data split. Either 'dev' or 'test'.")

    parser.add_argument(
        "work_dir", help="Working directory for all steps. "
                         "Will be created if doesn't exist.")

    parser.add_argument(
        "--comet_repo", help="Huggingface COMET model name. Must pass --comet_repo or --comet_path")

    parser.add_argument(
        "--comet_path", help="COMET model directory. Must pass --comet_repo or --comet_path")

    parser.add_argument(
        "--seed", type=int, default=0, help="Random seed for PyTorch.")

    parser.add_argument(
        "--comet_batch_size", type=int, default=128, help="COMET batch size.")

    parser.add_argument(
        "--mc_dropout", action="store_true", help="Activate MC dropout.")

    parser.add_argument(
        "--overwrite", action="store_true", help="Whether to overwrite existing data.")


    args = parser.parse_args()
    main(args)
